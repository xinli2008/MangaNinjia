# Update:
MangaNinjaçš„æ•´ä½“æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦åˆ†æ”¯ï¼šReference_Unetå’ŒDenoising_UNetã€‚æ–¹æ³•çš„å…³é”®æ­¥éª¤å¦‚ä¸‹:

1. æ•´ä½“æµç¨‹ï¼šé€šè¿‡éšæœºé€‰æ‹©è§†é¢‘ä¸­çš„ä¸¤å¸§ï¼Œä¸€å¸§ä½œä¸ºå‚è€ƒå›¾åƒï¼Œå¦ä¸€å¸§åŠå…¶çº¿ç¨¿ä½œä¸ºç›®æ ‡è¾“å…¥ã€‚è¿™ä¸¤å¸§åˆ†åˆ«è¾“å…¥åˆ°Reference_UNetå’ŒDenoising_UNetä¸­ã€‚Reference_Unetå’ŒDenoising_Uneté€šè¿‡self_attentionæœºåˆ¶æ¥å®Œæˆfeature fusionã€‚

2. Patch Unshuffleç­–ç•¥ï¼šå°†å‚è€ƒå›¾åƒåˆ†å‰²ä¸ºå¤šä¸ªå°patchå¹¶éšæœºé‡æ’ï¼Œè¿«ä½¿æ¨¡å‹åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å…³æ³¨å±€éƒ¨ç»†èŠ‚ï¼Œè€Œéå…¨å±€ç»“æ„ï¼Œå¢å¼ºå±€éƒ¨åŒ¹é…èƒ½åŠ›ã€‚è¿™ç§æ‰“ä¹±æ˜¯é€æ­¥å¢åŠ çš„ï¼Œå³Progressive Patch Unshuffleï¼Œä»2Ã—2åˆ°32Ã—32ï¼Œé‡‡ç”¨ä»ç²—åˆ°ç»†çš„å­¦ä¹ ç­–ç•¥ã€‚

3. ç‚¹é©±åŠ¨æ§åˆ¶æœºåˆ¶ï¼šç”¨æˆ·å¯ä»¥å®šä¹‰å‚è€ƒå›¾åƒå’Œçº¿æ¡è‰ºæœ¯ä¹‹é—´çš„åŒ¹é…ç‚¹ï¼Œåˆ©ç”¨PointNetæ¥å¢å¼ºæ¨¡å‹å¯¹è¿™äº›ç‚¹çš„æ„ŸçŸ¥ï¼Œä»è€Œå®ç°åŒºåŸŸå¯¹é½çš„ä¸Šè‰²ã€‚PointNetæ˜¯ä¸€ä¸ªç”±å¤šå±‚å·ç§¯å’ŒSiLUæ¿€æ´»å‡½æ•°ç»„æˆçš„ç½‘ç»œï¼Œç”¨äºå°†ç”¨æˆ·å®šä¹‰çš„ç‚¹å¯¹ç¼–ç ä¸ºå¤šå°ºåº¦åµŒå…¥ç‰¹å¾ã€‚è¿™äº›ç‰¹å¾é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä¸Denoising_Unetèåˆï¼Œå¢å¼ºæ¨¡å‹å¯¹ç”¨æˆ·æŒ‡å®šç‚¹çš„ç†è§£å’Œæ§åˆ¶èƒ½åŠ›ã€‚

4. è®­ç»ƒæµç¨‹ï¼šæ¨¡å‹çš„è®­ç»ƒåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€ä¸ªé˜¶æ®µä¸­ï¼Œæ¨¡å‹è®­ç»ƒReference_Unetã€Denoising_Unetå’ŒPointNetï¼Œè®­ç»ƒå†…å®¹åŒ…æ‹¬ç‰¹å¾èåˆï¼ŒPatch_Unshuffleå’Œæ¡ä»¶ä¸¢å¼ƒï¼ˆCondition_dropï¼‰ã€‚åœ¨ç¬¬äºŒä¸ªé˜¶æ®µï¼Œåªè®­ç»ƒPointNet, è¿›ä¸€æ­¥æå‡æ¨¡å‹å¯¹ç”¨æˆ·å®šä¹‰ç‚¹å¯¹çš„ç¼–ç å’Œæ„ŸçŸ¥èƒ½åŠ›ã€‚åœ¨MangaNinjaæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸¤å¼ å›¾ç‰‡ï¼ˆå‚è€ƒå›¾åƒå’Œç›®æ ‡å›¾åƒï¼‰ä¹‹é—´çš„åŒ¹é…ç‚¹æ˜¯é€šè¿‡ä¸€ä¸ª[LightGlue](https://github.com/cvg/LightGlue)ç‚¹åŒ¹é…ç®—æ³•å¾—åˆ°çš„ã€‚
-------------------------------------------
# MangaNinja: Line Art Colorization with Precise Reference Following

This repository represents the official implementation of the paper titled "MangaNinja: Line Art Colorization with Precise Reference Following".

[![Website](docs/badge-website.svg)](https://johanan528.github.io/MangaNinjia/)
[![Paper](https://img.shields.io/badge/arXiv-PDF-b31b1b)](https://arxiv.org/abs/2501.08332)
[![License](https://img.shields.io/badge/License-Apache--2.0-929292)](https://www.apache.org/licenses/LICENSE-2.0)

<p align="center">
    <a href="https://johanan528.github.io/"><strong>Zhiheng Liu*</strong></a>
    Â·
    <a href="https://felixcheng97.github.io/"><strong>Ka Leong Cheng*</strong></a>
    Â·
    <a href="https://xavierchen34.github.io/"><strong>Xi Chen</strong></a>
    Â·
    <a href="https://jiexiaou.github.io/"><strong>Jie Xiao</strong></a>
    Â·
    <a href="https://ken-ouyang.github.io/"><strong>Hao Ouyang</strong></a>
    Â·
    <a href="https://scholar.google.com/citations?user=Mo_2YsgAAAAJ&hl=zh-CN"><strong>Kai Zhu</strong></a>
    Â·
    <a href="https://scholar.google.com/citations?user=8zksQb4AAAAJ&hl=zh-CN"><strong>Yu Liu</strong></a>
    Â·
    <a href="https://shenyujun.github.io/"><strong>Yujun Shen</strong></a>
    Â·
    <a href="https://cqf.io/"><strong>Qifeng Chen</strong></a>
    Â·
    <a href="http://luoping.me/"><strong>Ping Luo</strong></a>
    <br>
  </p>

We propose **MangaNinja**, a reference-based line art colorization method. MangaNinja
automatically aligns the reference with the line art for colorization, demonstrating remarkable consistency. Additionally, users can achieve
more complex tasks using point control. We hope that MangaNinja can accelerate the colorization process in the anime industry.

![teaser](docs/teaser.gif)
## ğŸ“¢ News
* 2025-01-15: Inference code and paper are released.
* ğŸƒ: We will open an issue area to investigate user needs and adjust the model accordingly. This includes more memory-efficient structures, data formats for line art (such as binary line art), and considering retraining MangaNinjia on a better foundation model (sd3,flux).

## ğŸ› ï¸ Setup

### ğŸ“¦ Repository

Clone the repository (requires git):

```bash
git clone https://github.com/Johanan528/MangaNinjia.git
cd MangaNinjia
```

### ğŸ’» Dependencies

Install with `conda`: 
```bash
conda env create -f environment.yaml
conda activate MangaNinjia
```
### âš™ï¸ Weights
* You could download them from HuggingFace: [stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5), [clip image encoder](https://huggingface.co/openai/clip-vit-large-patch14), [line art controlnet](https://huggingface.co/lllyasviel/control_v11p_sd15_lineart) and [line art extractor](https://huggingface.co/lllyasviel/Annotators/blob/main/sk_model.pth)
* You could download our [MangaNinjia model](https://huggingface.co/Johanan0528/MangaNinjia) from HuggingFace 
* The downloaded checkpoint directory has the following structure:
```
`-- checkpoints
    |-- stable-diffusion-v1-5
    |-- clip image encoder
    |-- clip image encoder
    |-- line art controlnet
    |-- line art extractor
    `-- MangaNinjia
        |-- denoising_unet.pth
        |-- reference_unet.pth
        |-- point_net.pth
        |-- controlnet.pth
```


## ğŸ® Inference 
```bash
cd scripts
bash infer.sh
```

You can find all results in `output/`. Enjoy!

#### ğŸ“ Inference settings

The default settings are optimized for the best result. However, the behavior of the code can be customized:
  - `--denoise_steps`: Number of denoising steps of each inference pass. For the original (DDIM) version, it's recommended to use 20-50 steps.
  - `--is_lineart`: If the user provides an image and the task is to color the line art within that image, this parameter is not needed. However, if the input is already a line art and no additional extraction is necessary, then this parameter should be included.
  - `--guidance_scale_ref`: Increasing makes the model more inclined to accept the guidance of the reference image.
  - `--guidance_scale_point`: Increasing makes the model more inclined to input point guidance to achieve more customized colorization.
  - `--point_ref_paths` and `--point_lineart_paths` (**optional**): Two 512x512 matrices are used to represent the matching points between the corresponding reference and line art with continuously increasing integers. That is, the coordinates of the matching points in both matrices will have the same values: 1, 2, 3, etc., while the values in other positions will be 0 (you can refer to the provided samples). Of course, we recommend using Gradio for point guidance.

## ğŸŒ± Gradio
First, modify `./configs/inference.yaml` to set the path of model weight. Afterwards, run the script:
```bash
python run_gradio.py
```
The gradio demo would look like the UI shown below. 
<table align="center">
  <tr>
    <td>
      <img src="docs/gradio1.png" width="300" height="400">
    </td>
    <td>
      <img src="docs/gradio2.png" width="300" height="400">
    </td>
  </tr>
</table>
A biref tutorial:

1. Upload the reference image and target image. 

    Note that for the target image, there are two modes: you can upload an RGB image, and the model will automatically extract the line art; or you can directly upload the line art by checking the 'input is lineart' option. 

    The line art images are single-channel grayscale images, where the input consists of floating-point values with the background set to 0 and the line art close to 1. Additionally, we would like to further communicate with our users: if the line art you commonly use is binarized, please let us know. We will fine-tune the model and release an updated version to better suit your needs. ğŸ˜†

2. Click 'Process Images' to resize the images to 512*512 resolution.
3. (Optional) **Starting from the reference image**, **alternately** click on the reference and target images in sequence to define matching points. Use 'Undo' to revert the last action.
4. Click 'Generate' to produce the result.
## ğŸŒº Acknowledgements
This project is developped on the codebase of [MagicAnimate](https://github.com/magic-research/magic-animate). We appreciate this great work! 

## ğŸ“ Citation

Please cite our paper:

```bibtex
@article{liu2024manganinja,
  author  = {Zhiheng Liu and Ka Leong Cheng and Xi Chen and Jie Xiao and Hao Ouyang and Kai Zhu and Yu Liu and Yujun Shen
             and Qifeng Chen and Ping Luo},
  title   = {MangaNinja: Line Art Colorization with Precise Reference Following},
  journal = {CoRR},
  volume  = {abs/xxxx.xxxxx},
  year    = {2024}
}
```
